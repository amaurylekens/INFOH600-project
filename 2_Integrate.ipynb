{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import glob\n",
    "import json\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sub dataset, all the data are distributed on different files. As we have seen in part 1, for 3 sub-dataset (fvh, green, yellow) the pattern of the csv files has varied over time. This integration step will therefore make it possible to work later on data that have the same schema. \n",
    "\n",
    "In practice, for the 3 subdatasets, the most recent schema will be taken as the reference schema. All files with a different schema will be converted to their reference schema. This conversion can be either a simple change of column name but can also be a function of several columns.\n",
    "\n",
    "The first step will therefore be to identify these transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Identification of the transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3 following figures represent the correspondences between the different schemas of the same sub dataset. A reference schema is composed of the entries in the blue boxes. The names or functions below allow to retrieve or rebuild the data from a file built on a different schema. For example for the FHV subdataset the data _pulocationid_ can be found in a _pulocationid_ column but also in a _locationid_ column. In the green subdataset, this same data can also be found in the _pulocationid_ column but also by a function of the _pickup_longitude_ and _pickup_latitude_ columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/transformation_fhv.png\" width=\"800\" align=\"left\"/>\n",
    "<img src=\"img/transformation_green.png\" width=\"800\" align=\"left\"/>\n",
    "<img src=\"img/transformation_yellow.png\" width=\"800\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform these transformations, we decided to implement a very general function to avoid having to write a specific function for each dataset. The idea of this function is that it transforms a row of data not conforming to the reference scheme into a conforming row from a json configuration file. \n",
    "\n",
    "<img src=\"img/transformation_function.png\" width=\"800\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The json configuration file is subdataset specific, i.e. for each subdataset a configuration file must be defined. These configurations describe the hierarchical diagrams shown above. Part of one of these files (green dataset) is shown below. The whole files are in the repository in the _config_files_ folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "{ \n",
    " \"vendorid\": [\n",
    "              {\n",
    "               \"type\": \"column\", \n",
    "               \"content\": \"vendorid\"\n",
    "              },\n",
    "              {\n",
    "               \"type\": \"column\",\n",
    "               \"content\": \"vendor_id\"\n",
    "              }\n",
    "             ],\n",
    "\n",
    " ...\n",
    " \n",
    " \"store_and_fwd_flag\": [\n",
    "                        {\n",
    "                         \"type\": \"column\", \n",
    "                         \"content\": \"store_and_fwd_flag\"\n",
    "                        },\n",
    "                        {\n",
    "                         \"type\": \"column\", \n",
    "                         \"content\": \"store_and_forward\"\n",
    "                        }\n",
    "                       ],\n",
    "\n",
    " \"pulocationid\": [\n",
    "                  {\n",
    "                   \"type\": \"column\", \n",
    "                   \"content\": \"pulocationid\"\n",
    "                  },\n",
    "                  {\n",
    "                   \"type\": \"function\",\n",
    "                   \"content\": {\n",
    "                               \"func_name\": \"compute_location_id\",\n",
    "                               \"params\": [\"pickup_longitude\", \"pickup_latitude\"]\n",
    "                              }\n",
    "                  },\n",
    "                  {\n",
    "                   \"type\": \"function\",\n",
    "                   \"content\": {\n",
    "                               \"func_name\": \"compute_location_id\",\n",
    "                               \"params\": [\"start_lon\", \"start_lat\"]\n",
    "                              }\n",
    "                  }\n",
    "                 ], \n",
    "                         \n",
    " ...\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this json, the keys are the entries in the reference scheme and the values are _aliases_ of the key. An alias has a different name but represents in another file the same data as the key. There are several types of aliases :\n",
    "\n",
    "* *column* : the data is retrieved from another column whose name is specified by _content_.\n",
    "* *function* : the data is calculated from the data of several other columns. The name of the function is the name of columns-parameters are specified by _content_.\n",
    "\n",
    "The function will simply read the json and loop through its keys. For each key, it checks either that the column name or parameter names are in the wrong schema. If this is the case it retrieves the associated data and copies/calculates the value to be associated with the key in the new good schema.\n",
    "\n",
    "In our case, there will be column name changes and a single multi-column function that transforms geographic coordinates (latitude, longitude) into an area identifier. This function needs another parameter other than column names: a dataframe geopandas of geographical areas. The integration function must allow this parameter to be passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate(data, schema, integration_conf, params_f):\n",
    "\n",
    "    \"\"\"\n",
    "    Transforms data into the desired schema \n",
    "    by following the configuration\n",
    "\n",
    "    :param data: list of original data\n",
    "    :param schema: schema of original data\n",
    "    :param integration_conf: dict with the configuration\n",
    "    :param params_f: params for functions of columns\n",
    "    \"\"\"\n",
    "\n",
    "    data = dict(zip(schema, data))\n",
    "    t_data = dict() # transformed data\n",
    "\n",
    "    # loop through all the columns of the reference schema\n",
    "    for column, alias_list in integration_conf.items():\n",
    "        found = False\n",
    "\n",
    "        # loop through all alias (column or function)\n",
    "        for alias in alias_list:\n",
    "            category = alias['type']\n",
    "            content = alias['content']\n",
    "\n",
    "            # check category of the alias\n",
    "            # the alias is an other column name\n",
    "            if category == 'column':\n",
    "\n",
    "                # check that this name is in of the schema data\n",
    "                if content in list(data.keys()):\n",
    "                    t_data[column] = data[content]\n",
    "                    found = True\n",
    "                    break\n",
    "\n",
    "            # the alias is a function with other column name as param\n",
    "            elif category == 'function':\n",
    "\n",
    "                func_name = content['func_name']\n",
    "                param_names = content['params']\n",
    "                params = []\n",
    "\n",
    "                # check that all the params are in the schema of the data\n",
    "                eval_func = True\n",
    "                for param_name in param_names:\n",
    "                    if param_name not in list(data.keys()):\n",
    "                        eval_func = False\n",
    "                    else:\n",
    "                        params.append(data[param_name])\n",
    "\n",
    "                # eval the function if all the params are there\n",
    "                if eval_func :\n",
    "                    eval(\"dkdkdk\")\n",
    "                    \n",
    "                    \n",
    "        # if there is no valid alias add empty data\n",
    "        if not found:\n",
    "            t_data[column] = ''\n",
    "\n",
    "    data = list(t_data.values())\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest advantage of this function is its flexibility. I.e. if in the future a schema transformation is different (an extra column, changing date format, new taxi zone, ...), the function will remain valid. We have in fact simply separated the fixed part from the variable part of the transformation. The fixed part is the code of the function and the variable part is passed as a parameter.\n",
    "\n",
    "To make this flexibility possible, we use the python function _eval()_ which allows to evaluate python code passed as a string. For functions with several columns, a string \"name_of_the_function(param1, param2, ...)\" is constructed from the config file and passed to the _eval_ function.\n",
    "\n",
    "In the rest of the project, we will still have to implement functions that work with rows, to group these functions, we define a static Row class where each of these functions will be a static method. This class can be found in the _row.py_ file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implementation with spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme l'intégration d'un row est indépendant de l'intégration d'un autre row, on va pouvoir utiliser spark pour parraléliser cette opération. \n",
    "\n",
    "1. lecture le fichier csv dans HDFS\n",
    "2. application de la fonction _process_ aux rows : elle transforme un _string_ en _list_\n",
    "3. application la fonction _integrate_ aux rows\n",
    "4. application de la fonction _join_ aux rows : elle transforme une _list_ en _string_\n",
    "5. écriture des rows dans HDFS\n",
    "\n",
    "<img src=\"img/spark.png\" width=\"800\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HADOOP_CONF_DIR']=\"/etc/hadoop/conf\"\n",
    "\n",
    "os.environ['PYSPARK_PYTHON']=\"/usr/local/anaconda3/bin/python\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON']=\"/usr/local/anaconda3/bin/python\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "\n",
    "#remove old spark session\n",
    "try: \n",
    "    spark\n",
    "    print(\"Spark application already started. Terminating existing application and starting new one\")\n",
    "    spark.stop()\n",
    "except: \n",
    "    pass\n",
    "\n",
    "# Create a new spark session, with YARN as resource manager, requesting 4 worker nodes.\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.executor.instances\",\"4\") \\\n",
    "    .appName(\"project_ceci18\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#When dealing with RDDs, we work the sparkContext object. See https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark configuration\n",
    "sc.addFile(\"./row.py\")\n",
    "sc.addFile(\"./transformations.py\")\n",
    "sys.path.insert(0, SparkFiles.getRootDirectory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from row import Row\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "dataset = \"green\"\n",
    "\n",
    "# build dict with configurations\n",
    "integration_confs = dict()\n",
    "conf_filenames = sorted(glob.glob('/home/ceci18/INFOH600-project/integration_conf/*.json'))\n",
    "for conf_filename in conf_filenames:    \n",
    "    with open(conf_filename, 'r') as f:\n",
    "        integration_conf = json.load(f)\n",
    "        dataset_name = os.path.basename(conf_filename)[:-5]\n",
    "        integration_confs[dataset_name] = integration_conf\n",
    "\n",
    "# get all the filename\n",
    "hdfs_path = 'hdfs://public00:8020/user/hpda000034/infoh600/sampled'\n",
    "local_path = '/home/hpda00034/infoh600/sampled'\n",
    "filenames = sorted(glob.glob(\"{}/{}_*.csv\".format(local_path, dataset)))\n",
    "filenames = [os.path.basename(filename) for filename in filenames]\n",
    "\n",
    "# construct a rtree index with the geopanda df\n",
    "zones = gpd.read_file(\"./shape_files/taxi_zones.shp\")\n",
    "#zones.set_geometry('geometry', crs=(u'epsg:'+str(4326)), inplace=True)\n",
    "zones = zones.to_crs({'init':'epsg:4326'})\n",
    "\n",
    "# get the last schema of the dataset\n",
    "last_schema = list(integration_confs[dataset].keys())\n",
    "\n",
    "\n",
    "# Create one big rdd that holds all of the file's contents\n",
    "for filename in filenames:\n",
    "    print(filename)\n",
    "    # write the right schema on the first line\n",
    "    final_rdd = sc.parallelize([','.join(last_schema)])\n",
    "    \n",
    "    # get the schema of the file\n",
    "    file = open('{}/{}'.format(local_path, filename), 'r')\n",
    "    schema = file.readline().replace('\\n', '')\n",
    "    schema = schema.replace('\"','').replace(\"'\", '').lower().split(',')\n",
    "    \n",
    "    rdd = sc.textFile('{}/{}'.format(hdfs_path, filename))\n",
    "    # Each original file contains also the header (schema). \n",
    "    # We ignore this first line and convert everything else\n",
    "    rdd = rdd.zipWithIndex() \\\n",
    "             .filter(lambda x:x[1] > 1) \\\n",
    "             .repartition(4)\\\n",
    "             .map(lambda x: Row.process(x[0]))\\\n",
    "             .map(lambda x: Row.integrate(x, schema, integration_confs[dataset], zones))\\\n",
    "             .map(lambda x: Row.join(x))\n",
    "            # .map(lambda x: ','.join(x))\n",
    "    # add it to the rdd that we already have\n",
    "    final_rdd = final_rdd.union(rdd)\n",
    "\n",
    "    # saves this to HDFS in your home HDFS folder\n",
    "    final_rdd.saveAsTextFile('./integrated/{}/{}'.format(dataset, filename[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop spark\n",
    "try: \n",
    "    spark.stop()\n",
    "except: \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must then gather the partitions into a single file :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"green\"\n",
    "local_path = '/home/hpda00034/infoh600/sampled'\n",
    "filenames = sorted(glob.glob(\"{}/{}_*.csv\".format(local_path, dataset)))\n",
    "filenames = [os.path.basename(filename) for filename in filenames]\n",
    "\n",
    "for filename in filenames:\n",
    "    print(filename)\n",
    "    subprocess.run(['hadoop', 'fs', '-getmerge', './integrated/{}/{}/*'.format(dataset, filename[:-4]),\n",
    "                    '../integrated/{}/{}'.format(dataset, filename)])\n",
    "    subprocess.run(['hadoop', 'fs', '-rm', '-r', './integrated/{}/{}/'.format(dataset, filename[:-4])])\n",
    "    subprocess.run(['hadoop', 'fs', '-moveFromLocal', '../integrated/{}/{}'.format(dataset, filename),\n",
    "                    './integrated/{}/'.format(dataset)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sr_flag': 150349, 'dispatching_base_num': 13}\n",
      "{'../invalid_data/fhv/fhv_tripdata_2019-01.txt': 150357, '../invalid_data/fhv/fhv_tripdata_2019-02.txt': 5}\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "dataset = \"fhv\"\n",
    "path = \"../invalid_data\"\n",
    "filenames = sorted(glob.glob(\"{}/{}/*.txt\".format(path, dataset)))\n",
    "\n",
    "\n",
    "errors_by_file = {}\n",
    "errors_by_column = {}\n",
    "for filename in filenames:\n",
    "    f = open(filename, 'r')\n",
    "    lines = f.readlines()\n",
    "    lines = [eval(line.replace('\\n','')) for line in lines]\n",
    "    comma_indexs = [line[0].index(',') for line in lines]\n",
    "    lines = [(line[0][8:index], line[1]) for line, index in zip(lines, comma_indexs)]\n",
    "    count = sum([line[1] for line in lines])\n",
    "    \n",
    "    if count != 0:\n",
    "        errors_by_file[filename] = count\n",
    "    \n",
    "    for line in lines:\n",
    "        if line[0] not in errors_by_column.keys():\n",
    "            errors_by_column[line[0]] = line[1]\n",
    "        else:\n",
    "            errors_by_column[line[0]] += line[1]\n",
    "\n",
    "print(errors_by_column)\n",
    "print(errors_by_file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from row import Row\n",
    "from pandas_schema import Column, Schema\n",
    "import pandas_schema.validation as validation \n",
    "\n",
    "df = pd.read_csv('../green_tripdata_2013-09.csv')\n",
    "\n",
    "\n",
    "columns = ['vendorid', 'lpep_pickup_datetime', 'lpep_dropoff_datetime', 'store_and_fwd_flag', \n",
    "          'ratecodeid', 'pulocationid', 'dolocationid', 'passenger_count', 'trip_distance', \n",
    "          'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'ehail_fee', \n",
    "          'improvement_surcharge', 'total_amount', 'payment_type', 'trip_type', 'congestion_surcharge']\n",
    "\n",
    "\n",
    "df.columns = columns\n",
    "\n",
    "df.vendorid.head()\n",
    "\n",
    "\n",
    "# validation schema for green dataset\n",
    "schema_green = Schema([\n",
    "    Column('vendorid', \n",
    "           [validation.InListValidation([1, 2])], \n",
    "           allow_empty=True),\n",
    "    Column('lpep_pickup_datetime', \n",
    "           [validation.DateFormatValidation('%Y-%m-%d %H:%M:%S')],\n",
    "           allow_empty=True),\n",
    "    Column('lpep_dropoff_datetime', \n",
    "           [validation.DateFormatValidation('%Y-%m-%d %H:%M:%S')],\n",
    "           allow_empty=True),\n",
    "    Column('store_and_fwd_flag', \n",
    "           [validation.InListValidation([\"Y\", \"N\"])], \n",
    "           allow_empty=True),\n",
    "    Column('ratecodeid', \n",
    "           [validation.InListValidation([1, 2, 3, 4, 5, 6])], \n",
    "           allow_empty=True),\n",
    "    Column('pulocationid',\n",
    "           [validation.InRangeValidation(0, 266)],\n",
    "           allow_empty=True),\n",
    "    Column('dolocationid', \n",
    "           [validation.InRangeValidation(0, 266)],\n",
    "           allow_empty=True),\n",
    "    Column('passenger_count', \n",
    "           [validation.InRangeValidation(min=0)], \n",
    "           allow_empty=True),\n",
    "    Column('trip_distance', \n",
    "           [validation.InRangeValidation(min=0)], \n",
    "           allow_empty=True),\n",
    "    Column('fare_amount', \n",
    "           [validation.InRangeValidation(min=0)], \n",
    "           allow_empty=True),\n",
    "    Column('extra', \n",
    "           [validation.InRangeValidation(min=0)], \n",
    "           allow_empty=True),\n",
    "    Column('mta_tax', \n",
    "           [validation.InRangeValidation(min=0)], \n",
    "           allow_empty=True),\n",
    "    Column('tip_amount', \n",
    "           [validation.InRangeValidation(min=0)], \n",
    "           allow_empty=True),\n",
    "    Column('tolls_amount', \n",
    "           [validation.InRangeValidation(min=0)], \n",
    "           allow_empty=True),\n",
    "    Column('ehail_fee', \n",
    "           [validation.InRangeValidation(min=0)], \n",
    "           allow_empty=True),\n",
    "    Column('improvement_surcharge', \n",
    "           [validation.InRangeValidation(min=0)], \n",
    "           allow_empty=True),\n",
    "    Column('total_amount', \n",
    "           [validation.InRangeValidation(min=0)], \n",
    "           allow_empty=True),\n",
    "    Column('payment_type', \n",
    "           [validation.InListValidation([1, 2, 3, 4, 5, 6])], \n",
    "           allow_empty=True),\n",
    "    Column('trip_type', \n",
    "           [validation.InListValidation([1, 2])], \n",
    "           allow_empty=True),\n",
    "    Column('congestion_surcharge', \n",
    "           [validation.InRangeValidation(min=0)],\n",
    "           allow_empty=True)\n",
    "])\n",
    "\n",
    "errors = schema_green.validate(df)\n",
    "\n",
    "for error in errors:\n",
    "    print(error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
