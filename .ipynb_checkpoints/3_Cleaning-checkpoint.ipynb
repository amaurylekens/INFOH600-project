{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from row import Row\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf, trim\n",
    "from pyspark.sql.types import IntegerType, LongType, FloatType\n",
    "from pandas_schema import Column, Schema\n",
    "import pandas_schema.validation as validation \n",
    "\n",
    "from cleaning import validation_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step will allow you to remove invalid data from the datasets. An example of invalid data is a negative value for a data representing a distance. For each column in the 4 sub-datasets, assertion are maded based on the TLC specifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Identification of the assertions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/assertion_fhv_fhvhv.png\" width=\"800\" align=\"left\"/>\n",
    "<img src=\"img/assertion_green.png\" width=\"800\" align=\"left\"/>\n",
    "<img src=\"img/assertion_yellow.png\" width=\"800\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pandas-schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A python module that checks such assertion exists: _panda_schema_. This module is very easy to use, an example is given for the FHV dataset (the implementation for the other sub-datasets can be found in the _cleaning.py_ file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_schema import Column, Schema\n",
    "import pandas_schema.validation as validation \n",
    "\n",
    "# validation schema for fhv dataset\n",
    "schema_fhv = Schema([\n",
    "    Column('dispatching_base_num', \n",
    "           [validation.MatchesPatternValidation('^B[0-9]{5}$')], \n",
    "           allow_empty=True),\n",
    "    Column('pickup_datetime', \n",
    "           [validation.DateFormatValidation('%Y-%m-%d %H:%M:%S')],\n",
    "           allow_empty=True),\n",
    "    Column('dropoff_datetime', \n",
    "           [validation.DateFormatValidation('%Y-%m-%d %H:%M:%S')],\n",
    "           allow_empty=True),\n",
    "    Column('pulocationid',\n",
    "           [validation.InRangeValidation(1, 266)],\n",
    "           allow_empty=True),\n",
    "    Column('dolocationid', \n",
    "           [validation.InRangeValidation(1, 266)],\n",
    "           allow_empty=True),\n",
    "    Column('sr_flag', \n",
    "           [validation.InListValidation([1, None])],\n",
    "           allow_empty=True)\n",
    "])\n",
    "\n",
    "\n",
    "def validate(df, validation_schema):\n",
    "\n",
    "    \"\"\"\n",
    "    Validate the dataframe and return infos of the errors\n",
    "\n",
    "    :param df: df to validate\n",
    "    :param validation_schema: schema to validate the data \n",
    "    :return: list with errors info\n",
    "    \"\"\"\n",
    "\n",
    "    # validate the data\n",
    "    errors = validation_schema.validate(df)\n",
    "    errors = [[error.row, error.column, error.value] for error in errors]\n",
    "\n",
    "    return errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _validate_ function is a static method of the Row class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Basic cleaning operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some records contain errors that are not validity errors but display errors. These errors are :\n",
    "\n",
    "* trailing space: \"B0005 &nbsp; &nbsp; &nbsp;\"\n",
    "* leading space: \"&nbsp; &nbsp; &nbsp; B0005\"\n",
    "* int/float represented as a string: \"12\"\n",
    "* replace empty cell of numeric column by: 0\n",
    "\n",
    "This is done by loading each .csv file in a spark dataframe. We apply the following operations on the dataframe:\n",
    "\n",
    "* trim: to remove leading and trailing whithespace.\n",
    "* cast: to convert string to float/int\n",
    "* fillna: to fill the empty cells of some numeric columns by 0.\n",
    "\n",
    "A dictionary is used to specify on which columns to apply these last two operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Validation step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A version of pandas-schema that runs on the pyspark dataframe does not seem to exist at the moment. So we need to load the pyspark dataframe on the main node using the _toPandas()_ method. For large files the dataframe is too large for the main node's RAM. We didn't solve this problem but we considered two solutions:\n",
    "\n",
    "* increase the memory of the main node\n",
    "* split the file into several parts and reassemble it at the end\n",
    "\n",
    "When the dataframe is loaded on the main node, the validation scheme corresponding to the appropriate dataset is then applied. \n",
    "\n",
    "With the errors generated by this validation step, 2 things can be done:\n",
    "\n",
    "* Rows with errors are deleted.\n",
    "* Errors are saved to generate statistics on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark application already started. Terminating existing application and starting new one\n"
     ]
    }
   ],
   "source": [
    "os.environ['HADOOP_CONF_DIR']=\"/etc/hadoop/conf\"\n",
    "\n",
    "# python configuration\n",
    "os.environ['PYSPARK_PYTHON']=\"/usr/local/anaconda3/bin/python\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON']=\"/usr/local/anaconda3/bin/python\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles, SQLContext\n",
    "\n",
    "\n",
    "# remove old spark session\n",
    "try: \n",
    "    spark\n",
    "    print(\"Spark application already started. Terminating existing application and starting new one\")\n",
    "    spark.stop()\n",
    "except: \n",
    "    pass\n",
    "\n",
    "# Create a new spark session, with YARN as resource manager, requesting 4 worker nodes.\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.executor.instances\",\"4\") \\\n",
    "    .appName(\"project_ceci18\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a sql spark context\n",
    "sc=spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the cleaning configurations\n",
    "\n",
    "cleaning_conf = {\n",
    "    'fhv':{\n",
    "        'cast': {\n",
    "            'int': ['pulocationid', 'dolocationid', 'sr_flag']\n",
    "        },\n",
    "        'fill': {\n",
    "        }\n",
    "    },\n",
    "    'fhvhv':{\n",
    "        'cast': {\n",
    "            'int': ['pulocationid', 'dolocationid', 'sr_flag']\n",
    "        },\n",
    "        'fill': {\n",
    "        }\n",
    "    },\n",
    "    'green': {\n",
    "        'cast': {\n",
    "            'int': ['pulocationid', 'dolocationid', 'vendorid', 'ratecodeid', 'passenger_count',\n",
    "                    'payment_type', 'trip_type'],\n",
    "            'float': ['trip_distance', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', \n",
    "                      'tolls_amount', 'ehail_fee', 'improvement_surcharge', 'total_amount',\n",
    "                      'congestion_surcharge']\n",
    "        },\n",
    "        'fill': {\n",
    "            'extra':0, 'mta_tax':0, 'fare_amount':0, \n",
    "            'ehail_fee':0, 'tolls_amount':0\n",
    "        }\n",
    "    },\n",
    "    'yellow': {\n",
    "        'cast': {\n",
    "            'int': ['pulocationid', 'dolocationid', 'vendorid', 'ratecodeid', 'passenger_count',\n",
    "                    'payment_type'],\n",
    "            'float': ['trip_distance', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', \n",
    "                      'tolls_amount', 'improvement_surcharge', 'total_amount',\n",
    "                      'congestion_surcharge']\n",
    "        },\n",
    "        'fill': {\n",
    "            'extra':0, 'mta_tax':0, 'fare_amount':0, 'tolls_amount':0\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yellow_tripdata_2015-01.csv\n"
     ]
    }
   ],
   "source": [
    "dataset = \"yellow\"\n",
    "\n",
    "# get all the filename\n",
    "hdfs_path = 'hdfs://public00:8020/user/hpda000034/infoh600/clean'\n",
    "local_path = '/home/hpda00034/infoh600/sampled'\n",
    "filenames = sorted(glob.glob(\"{}/{}_*.csv\".format(local_path, dataset)))\n",
    "filenames = [os.path.basename(filename) for filename in filenames]\n",
    "\n",
    "# get the right cleaning configuration\n",
    "cast = cleaning_conf[dataset]['cast'] \n",
    "fill = cleaning_conf[dataset]['fill'] \n",
    "\n",
    "# list to store all the errors\n",
    "errors_df = []\n",
    "\n",
    "filenames = ['yellow_tripdata_2015-01.csv']\n",
    "for filename in filenames:\n",
    "    print(filename)\n",
    "    \n",
    "    # load the dataframe and fill empty cell\n",
    "    trips = sqlContext.read.csv(\"./integrated/{}/{}\".format(dataset, filename), \n",
    "                                header=True,\n",
    "                                inferSchema=True)\\\n",
    "                      .fillna(fill)\n",
    "    \n",
    "    # remove leading and traling whitespace\n",
    "    for column in trips.columns:\n",
    "        trips = trips.withColumn(column, trim(trips[column]))\n",
    "    \n",
    "    # cast numeric column\n",
    "    for type_, columns in cast.items():\n",
    "        for column in columns:\n",
    "            if type_ == 'int':\n",
    "                trips = trips.withColumn(column, trips[column].cast(IntegerType()))\n",
    "            elif type_ == 'float':\n",
    "                trips = trips.withColumn(column, trips[column].cast(FloatType()))\n",
    "\n",
    "    # load in the main node\n",
    "    df = trips.toPandas()\n",
    "    df.head()\n",
    "\n",
    "    # get errors for the current file\n",
    "    errors = [error + [filename] for error \n",
    "               in Row.validate(df, validation_schema[dataset])]\n",
    "    errors_df += errors\n",
    "    \n",
    "    # get dirty row\n",
    "    rows = [error[0] for error in errors]\n",
    "    \n",
    "    # remove dirty rows\n",
    "    if len(rows) != 0:\n",
    "        df = df.drop(rows, axis=0)\n",
    "        df.to_csv('../clean/{}/{}'.format('buffer', filename), index=False, header=True)\n",
    "    else:\n",
    "        df.to_csv('../clean/{}/{}'.format(dataset, filename), index=False, header=True)\n",
    "\n",
    "# save the errors\n",
    "columns = ['row', 'column', 'value', 'file']\n",
    "errors_df = pd.DataFrame(errors_df, columns=columns)\n",
    "errors_df.to_csv('../invalid_data/{}.csv'.format(dataset), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop spark\n",
    "try: \n",
    "    spark.stop()\n",
    "except: \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then send the cleaned-up files to HDFS.\n",
    "\n",
    "A problem was encountered during this step. When files with rows that have been deleted are saved in the same folder as files without deleted rows, transferring files with deleted rows to HDFS does not work (checksum problem). Therefore, these two types of files have been saved in different folders. This problem deserves further investigation, but we didn't have the time and we'll settle for this do-it-yourself solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -copyFromLocal /home/ceci18/clean/yellow/* ./clean/yellow\n",
    "!hadoop fs -copyFromLocal /home/ceci18/clean/buffer/* ./clean/yellow\n",
    "!rm /home/ceci18/clean/buffer/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will summarize the validation errors found. For each dataset we will represent the count of errors according to the different files and the count of the errors according to the different columns.\n",
    "\n",
    "NB: to see the graphs in better quality, you have to go to the figures folder.\n",
    "\n",
    "NB2: please note that the graphs do not show the number of rows deleted but the number of errors (there may be several errors per row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"green\"\n",
    "\n",
    "path = '/home/ceci18/invalid_data'\n",
    "\n",
    "df = pd.read_csv('{}/{}.csv'.format(path, dataset))\n",
    "\n",
    "if df.shape[0] > 0:\n",
    "    fig, (ax1, ax2) = plt.subplots(2,1, figsize = (26,22)) # yellow/green (26,22), fhv (20,13)\n",
    "    fig.tight_layout(pad=28.0)\n",
    "\n",
    "    errors_by_column = df.groupby(['column']).size()\n",
    "    errors_by_column.plot.bar(ax=ax1)\n",
    "    ax1.set_title('Count of errors by column', fontsize=18)\n",
    "    ax1.set_xlabel('column', fontsize=18)\n",
    "    ax1.set_ylabel('count', fontsize=18)\n",
    "    plt.setp(ax1.get_xticklabels(), rotation='vertical', fontsize=18)\n",
    "    plt.setp(ax1.get_yticklabels(), fontsize=18)\n",
    "    \n",
    "    errors_by_file = df.groupby(['file']).size()\n",
    "    errors_by_file.plot.bar(ax=ax2)\n",
    "    ax2.set_title('Count of errors by file', fontsize=18)\n",
    "    ax2.set_xlabel('file', fontsize=18)\n",
    "    ax2.set_ylabel('count', fontsize=18)\n",
    "    plt.setp(ax2.get_xticklabels(), rotation='vertical', fontsize=18)\n",
    "    plt.setp(ax2.get_yticklabels(), fontsize=18)\n",
    "    \n",
    "    fig.suptitle('{} dataset'.format(dataset.capitalize()), fontsize=26)\n",
    "    \n",
    "    plt.savefig('figures/errors_{}.png'.format(dataset))\n",
    "    plt.close()\n",
    "    \n",
    "else:\n",
    "    print(\"no error for {}\".format(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. FHV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/errors_fhv.png\" width=\"800\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vast majority of errors come from the **sr_flag** column.  Based on the TLC specifications, this entry can take the value 1 or be empty. Some rows had other values : ['4.0', '3.0', '2.0', '5.0', '6.0', '7.0', '13.0', '8.0', '10.0','9.0', '12.0', '16.0', '17.0', '11.0', '32.0', '15.0', '14.0','21.0', '20.0', '24.0', '22.0', '19.0', '18.0', '29.0','36.0', '41.0', '28.0']. All these mistakes were made in the same month : 01/2019. \n",
    "\n",
    "The other error found concerns the **dispatching_base_num** column. Some rows have the value '\\N'.\n",
    "\n",
    "These two types of errors cannot be repaired. The associated rows are therefore removed. A total of 150356 rows were removed in the _fhv_tripdata_2019-01.csv_ file and 5 in the _fhv_tripdata_2019-02.csv_ file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Green"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/errors_green.png\" width=\"800\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The errors for columns **congestion_surcharge**, **ehail_fee**, **extra**, **fare_amount**, **improvement_surcharge**, **mta_tax**, **passenger_count**, **tip_amount**, **tolls_amount**, **total_amount** and **trip_distance** are all of the same type: the entries must be positive but for some rows they are negative.\n",
    "\n",
    "For the **ratecodid** column, TLC specifies that the input must take an integer value from 1 to 6. But some entries have the value 99.\n",
    "\n",
    "A total of 6,667 rows were deleted on all the _green_ files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. Yellow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/errors_yellow.png\" width=\"800\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The errors for columns **congestion_surcharge**, **extra**, **fare_amount**, **improvement_surcharge**, **tip_amount**, **tolls_amount** and **total_amount** are all of the same type: the entries must be positive but for some rows they are negative.\n",
    "\n",
    "The **payment_type** column must take integer values from 1 to 6 but for some rows it has one of the following values : [192., 207., 161., 138., 98., 263., 131., 228., 237., 124., 69., 231., 149., 238., 48., 157., 74., 33., 190., 146., 50., 229., 245., 115., 222., 143., 45., 162., 230., 139., 165., 90., 252., 113., 152., 24., 76., 260., 159., 142., 61., 234., 82., 169., 108., 227., 163., 262., 186., 101., 235., 144., 79., 213., 164., 226., 35., 258., 133., 223., 141., 114., 145., 14., 181., 128., 36., 148., 88., 211., 103., 129., 233., 240., 206., 72., 232.,137., 170., 167., 120., 16., 249., 7., 91., 25., 175., 117., 209., 31., 86.]\n",
    "\n",
    "For the **ratecodid** column, TLC specifies that the input must take an integer value from 1 to 6. But some entries have the value 99.\n",
    "\n",
    "For the **vendorid** column, the entries must be equal to 1 or 2 but some entries have one of this values: [3., 4.]\n",
    "\n",
    "A total of 53127 rows were deleted on all the yellow files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
