{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from row import Row\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf, trim\n",
    "from pyspark.sql.types import IntegerType, LongType, FloatType\n",
    "from pandas_schema import Column, Schema\n",
    "import pandas_schema.validation as validation \n",
    "\n",
    "from cleaning import validation_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step will allow you to remove invalid data from the datasets. An example of invalid data is a negative value for a data representing a distance. For each column in the 4 sub-datasets, assertion are maded based on the TLC specifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Identification of the assertions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/assertion_fhv_fhvhv.png\" width=\"800\" align=\"left\"/>\n",
    "<img src=\"img/assertion_green.png\" width=\"800\" align=\"left\"/>\n",
    "<img src=\"img/assertion_yellow.png\" width=\"800\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pandas-schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A python module that checks such assertion exists: _panda_schema_. This module is very easy to use, an example is given for the FHV dataset (the implementation for the other sub-datasets can be found in the _cleaning.py_ file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_schema import Column, Schema\n",
    "import pandas_schema.validation as validation \n",
    "\n",
    "# validation schema for fhv dataset\n",
    "schema_fhv = Schema([\n",
    "    Column('dispatching_base_num', \n",
    "           [validation.MatchesPatternValidation('^B[0-9]{5}$')], \n",
    "           allow_empty=True),\n",
    "    Column('pickup_datetime', \n",
    "           [validation.DateFormatValidation('%Y-%m-%d %H:%M:%S')],\n",
    "           allow_empty=True),\n",
    "    Column('dropoff_datetime', \n",
    "           [validation.DateFormatValidation('%Y-%m-%d %H:%M:%S')],\n",
    "           allow_empty=True),\n",
    "    Column('pulocationid',\n",
    "           [validation.InRangeValidation(1, 266)],\n",
    "           allow_empty=True),\n",
    "    Column('dolocationid', \n",
    "           [validation.InRangeValidation(1, 266)],\n",
    "           allow_empty=True),\n",
    "    Column('sr_flag', \n",
    "           [validation.InListValidation([1, None])],\n",
    "           allow_empty=True)\n",
    "])\n",
    "\n",
    "\n",
    "def validate(data, schema, validation_schema):\n",
    "\n",
    "    \"\"\"\n",
    "    Validate the entries of the row with\n",
    "    the validation schema\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # validate the data\n",
    "    df = pd.DataFrame([data], columns=schema)\n",
    "    errors = validation_schema.validate(df)\n",
    "\n",
    "    validated = len(errors) == 0\n",
    "\n",
    "    return validated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the _validate_ function is a static method of the Row class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Basic cleaning operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some records contain errors that are not validity errors but display errors. These errors are :\n",
    "\n",
    "* trailing space: \"B0005    \"\n",
    "* leading space: \"    B0005\"\n",
    "* int/float represented as a string: \"12\"\n",
    "* replace empty cell of numeric column by: 0\n",
    "\n",
    "This is done by loading each .csv file in a spark dataframe . We apply the following operations:\n",
    "\n",
    "* trim: to remove leading and trailing whithespace.\n",
    "* cast: to convert string to float\n",
    "* fillna: to fill the empty cells in the numeric columns.\n",
    "\n",
    "A dictionary is used to specify on which columns to apply these last two operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Validation step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A version of pandas-schema that runs on the pyspark dataframe does not seem to exist at the moment. So we need to load the pyspark dataframe on the main node using the _toPandas()_ method. The validation scheme corresponding to the appropriate dataset is then applied. \n",
    "\n",
    "With the errors generated by the validation step, 2 things can be done:\n",
    "\n",
    "* Rows with errors are deleted.\n",
    "* Errors are saved to generate statistics on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HADOOP_CONF_DIR']=\"/etc/hadoop/conf\"\n",
    "\n",
    "# python configuration\n",
    "os.environ['PYSPARK_PYTHON']=\"/usr/local/anaconda3/bin/python\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON']=\"/usr/local/anaconda3/bin/python\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles, SQLContext\n",
    "\n",
    "\n",
    "# remove old spark session\n",
    "try: \n",
    "    spark\n",
    "    print(\"Spark application already started. Terminating existing application and starting new one\")\n",
    "    spark.stop()\n",
    "except: \n",
    "    pass\n",
    "\n",
    "# Create a new spark session, with YARN as resource manager, requesting 4 worker nodes.\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.executor.instances\",\"4\") \\\n",
    "    .appName(\"project_ceci18\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create spark context\n",
    "sc=spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the filename\n",
    "hdfs_path = 'hdfs://public00:8020/user/hpda000034/infoh600/clean'\n",
    "local_path = '/home/hpda00034/infoh600/sampled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_conf = {\n",
    "    'green': {\n",
    "        'cast': {\n",
    "            'int': ['pulocationid', 'dolocationid']\n",
    "        },\n",
    "        'fill': {\n",
    "            'extra':0, 'mta_tax':0, 'fare_amount':0, \n",
    "            'ehail_fee':0, 'tolls_amount':0\n",
    "        }\n",
    "    },\n",
    "    'fhv':{\n",
    "        'cast': {\n",
    "            'int': ['pulocationid', 'dolocationid', 'sr_flag']\n",
    "        },\n",
    "        'fill': {\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fhv_tripdata_2019-01.csv\n",
      "fhv_tripdata_2019-02.csv\n"
     ]
    }
   ],
   "source": [
    "dataset = \"fhv\"\n",
    "\n",
    "filenames = sorted(glob.glob(\"{}/{}_*.csv\".format(local_path, dataset)))\n",
    "filenames = [os.path.basename(filename) for filename in filenames]\n",
    "\n",
    "n_trips = []\n",
    "months = []\n",
    "\n",
    "cast = cleaning_conf[dataset]['cast'] \n",
    "fill = cleaning_conf[dataset]['fill'] \n",
    "\n",
    "errors_df = []\n",
    "filenames = ['fhv_tripdata_2019-01.csv', 'fhv_tripdata_2019-02.csv']\n",
    "for filename in filenames:\n",
    "    print(filename)\n",
    "    trips = sqlContext.read.csv(\"./integrated/{}/{}\".format(dataset, filename), \n",
    "                                header=True,\n",
    "                                inferSchema=True).fillna(fill)\n",
    "    \n",
    "    for column in trips.columns:\n",
    "        trips = trips.withColumn(column, trim(trips[column]))\n",
    "          \n",
    "    for type_, columns in cast.items():\n",
    "        for column in columns:\n",
    "            if type_ == 'int':\n",
    "                trips = trips.withColumn(column, trips[column].cast(IntegerType()))\n",
    "            elif type_ == 'float':\n",
    "                trips = trips.withColumn(column, trips[column].cast(FloatType()))\n",
    "    \n",
    "    df = trips.toPandas()\n",
    "    errors = [error + [filename] for error \n",
    "               in Row.validate(df, validation_schema[dataset])]\n",
    "    errors_df += errors\n",
    "    rows = [error[0] for error in errors]\n",
    "    \n",
    "    if len(rows) != 0:\n",
    "        df = df.drop(rows, axis=0)\n",
    "        df.to_csv('../clean/{}/{}'.format('buffer', filename), index=False)\n",
    "    else:\n",
    "        df.to_csv('../clean/{}/{}'.format('fhv', filename), index=False)\n",
    "    \n",
    "columns = ['row', 'column', 'value', 'file']\n",
    "errors_df = pd.DataFrame(errors_df, columns=columns)\n",
    "errors_df.to_csv('../invalid_data/{}.csv'.format(dataset), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop spark\n",
    "try: \n",
    "    spark.stop()\n",
    "except: \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -copyFromLocal /home/ceci18/clean/fhv/* ./clean/fhv\n",
    "!hadoop fs -copyFromLocal /home/ceci18/clean/buffer/* ./clean/fhv\n",
    "!rm /home/ceci18/clean/buffer/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will summarize the validation errors found. For each dataset we will represent the distribution of the errors according to the different files and the distribution of the errors according to the different columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"green\"\n",
    "\n",
    "path = '/home/ceci18/invalid_data'\n",
    "\n",
    "df = pd.read_csv('{}/{}.csv'.format(path, dataset))\n",
    "\n",
    "if df.shape[0] > 0:\n",
    "    fig, (ax1, ax2) = plt.subplots(2,1, figsize = (20,16))\n",
    "    fig.tight_layout(pad=20.0)\n",
    "\n",
    "    errors_by_column = df.groupby(['column']).size()\n",
    "    errors_by_column.plot.bar(ax=ax1)\n",
    "    ax1.set_title('Count of errors by column')\n",
    "    ax1.set_ylabel('count')\n",
    "\n",
    "    errors_by_file = df.groupby(['file']).size()\n",
    "    errors_by_file.plot.bar(ax=ax2)\n",
    "    ax2.set_title('Count of errors by file')\n",
    "    ax2.set_ylabel('count')\n",
    "\n",
    "    plt.savefig('figures/errors_{}.png'.format(dataset))\n",
    "    plt.close()\n",
    "    \n",
    "else:\n",
    "    print(\"no error for {}\".format(dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
